{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "import h5py\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer, Add, Multiply, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from pickle import load\n",
    "from IPython import display\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "latent_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = './model/vae_decoder.json'\n",
    "model_weights = './model/vae_decoder.h5'\n",
    "sampler_file = './model/vae_sampler.json'\n",
    "sampler_weights = './model/vae_sampler.h5'\n",
    "z_meta_file = './model/z_meta.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tanliangwei/Desktop/fyp_repo/tf_1/lib/python3.6/site-packages/tensorflow_core/python/ops/linalg/linear_operator_lower_triangular.py:158: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n"
     ]
    }
   ],
   "source": [
    "json_file = open(model_file, 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "decoder = model_from_json(loaded_model_json)\n",
    "decoder.load_weights(model_weights)\n",
    "\n",
    "sampler = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=(tfpl.MultivariateNormalTriL.params_size(latent_dim), ), name = 'input_x'),\n",
    "    tfpl.MultivariateNormalTriL( latent_dim, name = 'sample_layer'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_meta= np.load(z_meta_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_categorical_map = {0:0, 1:0.5, 2:1, 3:1.5, 4:2, 5:2.5, 6:3, 7:3.5, 8:4, 9:4.5, 10:5}\n",
    "def sample(decoder, sampler, input_z_params,samples_per_z=1):\n",
    "    multiplied_input_z_params = np.repeat(input_z_params, samples_per_z, axis=0)\n",
    "    z = sampler.predict(multiplied_input_z_params)\n",
    "    predictions = decoder.predict(z, batch_size = None, steps = 1)\n",
    "    return reconstruct(predictions)\n",
    "    \n",
    "def reconstruct(predictions):\n",
    "    coordinates, review, categorical, binary = predictions\n",
    "    \n",
    "    # coordinates handled here\n",
    "    mu, log_var = np.split(coordinates, indices_or_sections = 2,axis = 1)\n",
    "    eps = np.random.normal(size=mu.shape)\n",
    "    coordinates_data = reparameterize(mu, log_var, eps)\n",
    "    scaler = load(open('./model/standard_scaler.pkl', 'rb'))\n",
    "    coordinates_data = scaler.inverse_transform(coordinates_data)\n",
    "    \n",
    "    for i, c in enumerate(coordinates_data):\n",
    "        if c[0] > 180.0:\n",
    "            coordinates_data[i][0]= 180.0\n",
    "        if c[0] < -180.0:\n",
    "            coordinates_data[i][0]= -180.0\n",
    "        if c[1] > 180.0:\n",
    "            coordinates_data[i][1]= 180.0\n",
    "        if c[1] < -180.0:\n",
    "            coordinates_data[i][1]= -180.0\n",
    "    \n",
    "    ## review_count handled here\n",
    "    exp_log_review = np.exp(review)\n",
    "    review_data = np.random.poisson(lam=exp_log_review, size = review.shape)\n",
    "    for i, r in enumerate(review_data):\n",
    "        if r[0] < 0:\n",
    "            review_data[i][0] = 0\n",
    "        review_data[i][0] = float(int(review_data[i][0]))\n",
    "    \n",
    "    categorical = np.apply_along_axis(lambda t : np.random.multinomial(1,t), -1, categorical)\n",
    "    categorical = np.apply_along_axis(lambda t : np.argmax(t), -1, categorical)\n",
    "    categorical = np.expand_dims(categorical, axis = -1)\n",
    "    categorical_data = np.apply_along_axis(lambda t : float(reverse_categorical_map[t[0]]), -1, categorical)\n",
    "    categorical_data = np.expand_dims(categorical_data, axis = -1)\n",
    "    binary_data = np.apply_along_axis(lambda t: np.random.binomial(1, t), -1, binary)\n",
    "#     coordinates, reviews = np.split(continuous_data, indices_or_sections=[2], axis = 1)\n",
    "    return np.concatenate([coordinates_data, categorical_data, review_data, binary_data], axis = 1)\n",
    "    \n",
    "\n",
    "def reparameterize(input_mu, input_log_var, eps):\n",
    "    sigma = np.exp(0.5*input_log_var)\n",
    "    return eps*sigma + input_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Generation of Samples\n",
    "\n",
    "The snippet (sample function) allows you to generate samples. You will need to supply the \n",
    "\n",
    "1. vae model\n",
    "2. array of mu\n",
    "3. array of log_var\n",
    "4. the number of samples you wish to generate per mu (optional and defaults to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_samples = sample(decoder, sampler, z_meta, 2)\n",
    "file_name = './samples/vae_6_2_sample_' + str(len(z_meta))+'_times_'+ str(len(vae_samples)//len(z_meta)) + '_test.csv'\n",
    "np.savetxt(file_name, vae_samples, delimiter = ',', header='latitude,longitude,stars,review_count,is_open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
