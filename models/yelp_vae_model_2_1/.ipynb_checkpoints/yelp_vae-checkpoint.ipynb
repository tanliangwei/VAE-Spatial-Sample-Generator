{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None)\n",
      "(None, 11)\n",
      "(None, None)\n",
      "(None, 6)\n",
      "Train on 174565 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      "174565/174565 [==============================] - 10s 55us/step - loss: 800.5119 - x_pred_binary_loss: 0.4785 - x_pred_categorical_loss: 2.0988 - x_pred_continuous_loss: 790.8143 - val_loss: 20.6080 - val_x_pred_binary_loss: 0.1743 - val_x_pred_categorical_loss: 1.7761 - val_x_pred_continuous_loss: 15.6916\n",
      "Epoch 2/2\n",
      "174565/174565 [==============================] - 12s 67us/step - loss: 21.5535 - x_pred_binary_loss: 0.4429 - x_pred_categorical_loss: 2.0101 - x_pred_continuous_loss: 16.4278 - val_loss: 20.4279 - val_x_pred_binary_loss: 0.1814 - val_x_pred_categorical_loss: 1.7913 - val_x_pred_continuous_loss: 16.1694\n",
      "Saved model to disk\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "import h5py\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Add, Multiply\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "categorical_dim = 11\n",
    "categorical_map = {0:0, 0.5:1, 1: 2, 1.5:3, 2:4, 2.5:5, 3:6, 3.5:7, 4:8, 4.5:9, 5:10}\n",
    "reverse_categorical_map = {0:0, 1:0.5, 2:1, 3:1.5, 4:2, 5:2.5, 6:3, 7:3.5, 8:4, 9:4.5, 10:5}\n",
    "continuous_dim = 3\n",
    "binary_dim = 1\n",
    "original_dim = binary_dim + continuous_dim + categorical_dim\n",
    "intermediate_dim_1 = 50\n",
    "intermediate_dim_2 = 50\n",
    "latent_dim = 4\n",
    "batch_size = 100\n",
    "epochs = 2\n",
    "epsilon_std = 1.0\n",
    "\n",
    "# Layer Definition\n",
    "\n",
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss((kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "# Loss functions\n",
    "def binary_loss(y_true, y_pred):\n",
    "\t# input dimension is (batchsize, 1)\n",
    "  return K.binary_crossentropy(y_true, y_pred) # the dimension of return value is (batchsize , 1)\n",
    "\n",
    "def categorical_loss(y_true, y_pred):\n",
    "\t# input dimension is (batchsize, number of categories)\n",
    "\t# now, turn y_true into one hot.\n",
    "  print(y_true.shape)\n",
    "  print(y_pred.shape)\n",
    "  return K.categorical_crossentropy(y_true, y_pred)\n",
    "  # returning (batchsize, 1)\n",
    "\n",
    "# to aid in computation of continuous loss\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "  log2pi = tf.math.log(2. * np.pi)\n",
    "  return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis) # return a tensor of shape (batch_size, 1)\n",
    "\n",
    "def continuous_loss(y_true, y_pred):\n",
    "\t# need to return log probability for continuous loss.\n",
    "\t# will get a (batchsize, 6 continuous variable input) where 3 of the 6 represents mu and the others logvar\n",
    "\t# y_true will be (batchsize, 3)\n",
    "  mu, logvar = tf.split(y_pred, num_or_size_splits = 2, axis = 1)\n",
    "  print(y_true.shape)\n",
    "  print(y_pred.shape)\n",
    "  # a = log_normal_pdf(y_true, mu, logvar) \n",
    "  # print(a.shape)\n",
    "  # shape is (batch_size, 1) or just (batch_size, )\n",
    "  return -1 * log_normal_pdf(y_true, mu, logvar) \n",
    "\n",
    "\n",
    "# Model Architecture\n",
    "x = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim_1, activation='tanh')(x)\n",
    "h = Dense(intermediate_dim_2, activation='tanh')(h)\n",
    "\n",
    "z_mu = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n",
    "z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)\n",
    "\n",
    "eps = Input(name = 'abc', tensor=K.random_normal(stddev=epsilon_std,\n",
    "                                   shape=(K.shape(x)[0], latent_dim)))\n",
    "z_eps = Multiply()([z_sigma, eps])\n",
    "z = Add()([z_mu, z_eps])\n",
    "\n",
    "decode_1 = Dense(intermediate_dim_2, activation='tanh', \n",
    "              name='hidden_dec_2')\n",
    "h_dec = decode_1(z)\n",
    "\n",
    "decode_2 = Dense(intermediate_dim_1, activation='tanh', \n",
    "              name='hidden_dec_1')\n",
    "h_dec = decode_2(h_dec)\n",
    "\n",
    "x_pred_continuous_layer = Dense(continuous_dim*2, name='x_pred_continuous')\n",
    "x_pred_continuous = x_pred_continuous_layer(h_dec) # mu\n",
    "#x_pred_continuous_logvar = Dense(continuous_dim, name='x_pred_continuous_logvar')(h_dec) # logvar\n",
    "x_pred_binary_layer = Dense(binary_dim, activation='sigmoid', name='x_pred_binary')\n",
    "x_pred_binary = x_pred_binary_layer(h_dec) # binary cross entropy\n",
    "\n",
    "x_pred_categorical_layer = Dense(categorical_dim, activation='softmax', name='x_pred_categorical')\n",
    "x_pred_categorical = x_pred_categorical_layer(h_dec) # categorical cross entropy\n",
    "\n",
    "# Creating the Model\n",
    "vae = Model(inputs=[x,eps], outputs=[x_pred_binary, x_pred_categorical, x_pred_continuous])\n",
    "# vae = Model(inputs=[x,eps], outputs=[x_pred_continuous, x_pred_binary, x_pred_categorical])\n",
    "\n",
    "# vae.compile(optimizer='rmsprop', loss=[continuous_loss, binary_loss, categorical_loss], loss_weights=[ 1, 1, 1])\n",
    "optimizer = keras.optimizers.Adam(lr=0.0001,clipvalue=1)\n",
    "vae.compile(optimizer=optimizer, loss=[binary_loss, categorical_loss, continuous_loss], loss_weights=[1, 1, 1])\n",
    "\n",
    "\n",
    "# Data Loading and preprocessing\n",
    "import numpy as np\n",
    "dataset = np.genfromtxt('yelp_business.csv',delimiter=',',skip_header=1)\n",
    "dataset = dataset[~np.isnan(dataset).any(axis=1)]\n",
    "test_set, train_set = np.split(dataset,[1], axis = 0)\n",
    "# coordinates, ratings, reviews, is_opens = np.split(dataset, [2, 3, 4], axis = 1)\n",
    "# one_hot_array = np.zeros((ratings.shape[0], categorical_dim))\n",
    "\n",
    "# for i, r in enumerate(ratings):\n",
    "#   one_hot_array[i][categorical_map[r[0]]] = 1\n",
    "\n",
    "# dataset = np.concatenate((coordinates, reviews, is_opens, one_hot_array), axis = 1)\n",
    "\n",
    "# # creating the labels\n",
    "# continuous_labels = np.concatenate((coordinates, reviews), axis = 1)\n",
    "# categorical_labels = one_hot_array\n",
    "# binary_labels = is_opens\n",
    "\n",
    "\n",
    "\n",
    "def format_data(dataset):\n",
    "  # handling the categorical variables\n",
    "  coordinates, ratings, reviews, is_opens = np.split(dataset, [2, 3, 4], axis = 1)\n",
    "  one_hot_array = np.zeros((ratings.shape[0], categorical_dim))\n",
    "\n",
    "  for i, r in enumerate(ratings):\n",
    "    one_hot_array[i][categorical_map[r[0]]] = 1\n",
    "\n",
    "  dataset = np.concatenate((coordinates, reviews, is_opens, one_hot_array), axis = 1)\n",
    "\n",
    "  # creating the labels\n",
    "  continuous_labels = np.concatenate((coordinates, reviews), axis = 1)\n",
    "  categorical_labels = one_hot_array\n",
    "  binary_labels = is_opens\n",
    "  return dataset, continuous_labels, categorical_labels, binary_labels\n",
    "\n",
    "train_dataset, train_continuous_labels, train_categorical_labels, train_binary_labels = format_data(train_set)\n",
    "test_dataset, test_continuous_labels, test_categorical_labels, test_binary_labels = format_data(test_set)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "# vae.fit(dataset, [continuous_labels, binary_labels, categorical_labels], shuffle = True, epochs = epochs, batch_size = batch_size)#, validation_data = (train_dataset, [train_continuous_labels, train_binary_labels, train_categorical_labels]))\n",
    "vae.fit(train_dataset , [train_binary_labels, train_categorical_labels, train_continuous_labels], shuffle = True, epochs = epochs, batch_size = batch_size, validation_data=(test_dataset , [test_binary_labels, test_categorical_labels, test_continuous_labels]))\n",
    "\n",
    "# Saving the Model\n",
    "model_json = vae.to_json()\n",
    "with open(\"full_mode.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "print(\"Saved model to disk\")\n",
    "vae.save_weights(\"full_model.h5\")\n",
    "\n",
    "\n",
    "# Getting the Encoder (Saving the Model)\n",
    "encoder = Model(x, [z_mu, z_log_var])\n",
    "model_json = encoder.to_json()\n",
    "with open(\"full_mode.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "print(\"Saved model to disk\")\n",
    "encoder.save_weights(\"encoder.h5\")\n",
    "\n",
    "\n",
    "# Generating and Storing the Metadata\n",
    "vae_sample = train_dataset[np.random.choice(len(train_dataset), size=10000, replace=False)]\n",
    "vae_sample_mu, vae_sample_log_var = encoder.predict(vae_sample, batch_size=batch_size)\n",
    "np.save('sample_mu.npy', vae_sample_mu)\n",
    "np.save('sample_log_var.npy', vae_sample_log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_input = Input(shape=(latent_dim, ), name = 'decode_input')\n",
    "decode_layer_1 = decode_1(decode_input)\n",
    "decode_layer_2 = decode_2(decode_layer_1)\n",
    "decode_x_pred_continuous = x_pred_continuous_layer(decode_layer_2) # mu\n",
    "#x_pred_continuous_logvar = Dense(continuous_dim, name='x_pred_continuous_logvar')(h_dec) # logvar\n",
    "decode_x_pred_binary = x_pred_binary_layer(decode_layer_2) # binary cross entropy\n",
    "decode_x_pred_categorical = x_pred_categorical_layer(decode_layer_2) # categorical cross entropy\n",
    "\n",
    "\n",
    "decoder = Model(decode_input, [decode_x_pred_continuous, decode_x_pred_categorical, decode_x_pred_binary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = decoder.to_json()\n",
    "with open(\"decoder.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "print(\"Saved model to disk\")\n",
    "decoder.save_weights(\"decoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -2.74844665, -44.12384464,   3.        ,   7.08223282,\n",
       "          1.        ],\n",
       "       [-20.30966713, -15.58434975,   4.        ,  43.20102221,\n",
       "          1.        ]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_categorical_map = {0:0, 1:0.5, 2:1, 3:1.5, 4:2, 5:2.5, 6:3, 7:3.5, 8:4, 9:4.5, 10:5}\n",
    "def sample(model, input_mu, input_log_var):\n",
    "    eps = np.random.normal(size=(input_mu.shape[0], latent_dim))\n",
    "    z = reparameterize(input_mu, input_log_var, eps)\n",
    "    predictions = model.predict(z, batch_size = None, steps = 1)\n",
    "    return reconstruct(predictions)\n",
    "    \n",
    "def reconstruct(predictions):\n",
    "    continuous, categorical, binary = predictions\n",
    "    mu, log_var = np.split(continuous, indices_or_sections = 2,axis = 1)\n",
    "    eps = np.random.normal(size=mu.shape)\n",
    "    continuous_data = reparameterize(mu, log_var, eps)\n",
    "    categorical = np.apply_along_axis(lambda t : np.random.multinomial(1,t), -1, categorical)\n",
    "    categorical = np.apply_along_axis(lambda t : np.argmax(t), -1, categorical)\n",
    "    categorical = np.expand_dims(categorical, axis = -1)\n",
    "    categorical_data = np.apply_along_axis(lambda t : reverse_categorical_map[t[0]], -1, categorical)\n",
    "    categorical_data = np.expand_dims(categorical_data, axis = -1)\n",
    "    binary_data = np.apply_along_axis(lambda t: np.random.binomial(1, t), -1, binary)\n",
    "    coordinates, reviews = np.split(continuous_data, indices_or_sections=[2], axis = 1)\n",
    "    return np.concatenate([coordinates, categorical_data, reviews, binary_data], axis = 1)\n",
    "    \n",
    "\n",
    "def reparameterize(input_mu, input_log_var, eps):\n",
    "    sigma = np.exp(.5*input_log_var)\n",
    "    return eps*sigma + input_mu\n",
    "\n",
    "sample(decoder, vae_sample_mu[2:4], vae_sample_log_var[2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
