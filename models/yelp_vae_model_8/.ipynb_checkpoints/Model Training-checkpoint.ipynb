{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "In this notebook, we will train our VAE model. This involves:\n",
    "\n",
    "1. Encoder\n",
    "2. Decoder\n",
    "3. Full Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages\n",
    "We will be using Keras to build and train our VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras.utils.vis_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f0e1f2c458e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFormatStrFormatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.utils.vis_utils'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "%matplotlib notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "import h5py\n",
    "\n",
    "# import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer, Add, Multiply, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import model_from_json\n",
    "# import mdn\n",
    "\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from pickle import dump, load\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Constants\n",
    "Hyper parameters and constants will all be located here for ease of adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_dim = 11\n",
    "categorical_map = {0:0, 0.5:1, 1: 2, 1.5:3, 2:4, 2.5:5, 3:6, 3.5:7, 4:8, 4.5:9, 5:10}\n",
    "reverse_categorical_map = {0:0, 1:0.5, 2:1, 3:1.5, 4:2, 5:2.5, 6:3, 7:3.5, 8:4, 9:4.5, 10:5}\n",
    "continuous_dim = 3\n",
    "binary_dim = 1\n",
    "original_dim = binary_dim + continuous_dim + categorical_dim\n",
    "intermediate_dim_1 = 50\n",
    "intermediate_dim_2 = 50\n",
    "latent_dim = 4\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "epsilon_std = 1.0\n",
    "\n",
    "## Constants for the Mixture layer\n",
    "N_HIDDEN = 15  # number of hidden units in the Dense layer\n",
    "N_MIXES = 10  # number of mixture components\n",
    "OUTPUT_DIMS = 2  # number of real-values predicted by each mixture component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "We have to write our own custom layer and custom loss function as these are not supported on Keras natively. There are a few things to be done:\n",
    "\n",
    "1. Custom KLDivergence Layer\n",
    "2. Custom Loss Functions\n",
    "3. Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence Layer\n",
    "To ensure modularity, we decided to create a separate layer for KL Divergence. This layer will account for the loss required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss Functions\n",
    "As the yelp dataset contains binary, categorical as well as continuous data, we will build 3 custom loss functions.\n",
    "\n",
    "#### Binary Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_loss(y_true, y_pred):\n",
    "\t# input dimension is (batchsize, 1)\n",
    "    return K.binary_crossentropy(y_true, y_pred) # the dimension of return value is (batchsize , 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_loss(y_true, y_pred):\n",
    "\t# input dimension is (batchsize, number of categories)\n",
    "  return K.categorical_crossentropy(y_true, y_pred) # the dimension of return value is (batchsize , 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "  log2pi = tf.math.log(2. * np.pi)\n",
    "  return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis) # return a tensor of shape (batch_size, 1)\n",
    "\n",
    "# Added a postfix because now we also have poisson as a distribution\n",
    "def continuous_loss_gaussian(y_true, y_pred):\n",
    "\t# need to return log probability for continuous gaussian loss.\n",
    "\t# will get a (batchsize, 6 continuous variable input) where 3 of the 6 represents mu and the others logvar\n",
    "\t# y_true will be (batchsize, 3)\n",
    "  mu, logvar = tf.split(y_pred, num_or_size_splits = 2, axis = 1)\n",
    "  return -1 * log_normal_pdf(y_true, mu, logvar) \n",
    "\n",
    "def continuous_loss_poisson(y_true, y_pred):\n",
    "\t# need to return log probability for continuous poisson loss.\n",
    "\t# will get a (batchsize, 6 continuous variable input) where 3 of the 6 represents mu and the others logvar\n",
    "\t# y_true will be (batchsize, 3)\n",
    "  return tf.nn.log_poisson_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "- tanh/sigmoid is used because Relu resulted in loss going to infinity\n",
    "- going to delete review_log_var and review_mu since we are trying out the Poission distribution which only takes 1 parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "import mdn\n",
    "\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "input_shape = (original_dim,)\n",
    "base_depth = 32\n",
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(latent_dim), scale=1), reinterpreted_batch_ndims=1)\n",
    "\n",
    "## Encoder\n",
    "encoder = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=input_shape),\n",
    "    tfkl.Dense(intermediate_dim_1, activation='tanh', name='hidden_enc_1'),\n",
    "    tfkl.Dense(intermediate_dim_2, activation='tanh', name='hidden_enc_2'),\n",
    "    tfkl.Dense(tfpl.MultivariateNormalTriL.params_size(latent_dim), activation=None),\n",
    "    tfpl.MultivariateNormalTriL( latent_dim, activity_regularizer=tfpl.KLDivergenceRegularizer(prior)),\n",
    "])\n",
    "\n",
    "\n",
    "decode_1 = tfkl.Dense(intermediate_dim_2, activation='tanh', name='hidden_dec_2')\n",
    "h_dec = decode_1(encoder.outputs[0])\n",
    "\n",
    "decode_2 = tfkl.Dense(intermediate_dim_1, activation='tanh', name='hidden_dec_1')\n",
    "h_dec = decode_2(h_dec)\n",
    "\n",
    "x_pred_coordinates_mdn_layer = mdn.MDN(OUTPUT_DIMS, N_MIXES, name = 'x_pred_coordinates_mdn_layer')\n",
    "x_pred_coordinates = x_pred_coordinates_mdn_layer(h_dec)\n",
    "\n",
    "## Old coordinates layer\n",
    "# x_pred_coordinates_mu_layer = tfkl.Dense(2, name='x_pred_coordinates_mu')\n",
    "# x_pred_coordinates_mu = x_pred_coordinates_mu_layer(h_dec)\n",
    "\n",
    "# x_pred_coordinates_log_var_layer = tfkl.Dense(2, name='x_pred_coordinates_log_var')\n",
    "# x_pred_coordinates_log_var = x_pred_coordinates_log_var_layer(h_dec)\n",
    "\n",
    "# x_pred_coordinates_layer = tfkl.Concatenate(axis=-1, name = 'x_pred_coordinates')\n",
    "# x_pred_coordinates = x_pred_coordinates_layer([x_pred_coordinates_mu, x_pred_coordinates_log_var])\n",
    "\n",
    "## This outputs the lambda require for poisson distribution and thats all that we need\n",
    "x_pred_review_log_lambda_layer = tfkl.Dense(1,name = 'x_pred_review_log_lambda_layer')\n",
    "x_pred_review = x_pred_review_log_lambda_layer(h_dec)\n",
    "\n",
    "x_pred_binary_layer = tfkl.Dense(binary_dim, activation='sigmoid', name='x_pred_binary')\n",
    "x_pred_binary = x_pred_binary_layer(h_dec) # binary cross entropy\n",
    "\n",
    "x_pred_categorical_layer = tfkl.Dense(categorical_dim, activation='softmax', name='x_pred_categorical')\n",
    "x_pred_categorical = x_pred_categorical_layer(h_dec) # categorical cross entropy\n",
    "\n",
    "vae = tfk.Model(inputs=encoder.inputs, outputs=[x_pred_binary, x_pred_categorical, x_pred_review, x_pred_coordinates])\n",
    "# vae = tfk.Model(inputs=encoder.inputs,\n",
    "#                 outputs=decoder(encoder.outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling Model and Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "vae.compile(optimizer=optimizer, loss=[binary_loss, categorical_loss, continuous_loss_poisson, mdn.get_mixture_loss_func(OUTPUT_DIMS,N_MIXES)], loss_weights=[1, 1, 1, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden_enc_1 (Dense)            (None, 50)           800         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hidden_enc_2 (Dense)            (None, 50)           2550        hidden_enc_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 14)           714         hidden_enc_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "multivariate_normal_tri_l_3 (Mu ((None, 4), (None, 4 0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hidden_dec_2 (Dense)            (None, 50)           250         multivariate_normal_tri_l_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "hidden_dec_1 (Dense)            (None, 50)           2550        hidden_dec_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "x_pred_binary (Dense)           (None, 1)            51          hidden_dec_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "x_pred_categorical (Dense)      (None, 11)           561         hidden_dec_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "x_pred_review_log_lambda_layer  (None, 1)            51          hidden_dec_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "x_pred_coordinates_mdn_layer (M (None, 50)           2550        hidden_dec_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 10,077\n",
      "Trainable params: 10,077\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.genfromtxt('../../datasets/yelp_business.csv',delimiter=',',skip_header=1)\n",
    "dataset = dataset[~np.isnan(dataset).any(axis=1)]\n",
    "test_set, train_set = np.split(dataset,[1], axis = 0)\n",
    "\n",
    "def format_data(dataset, scaler_to_use=None ,save_scaler=True):\n",
    "    # handling the categorical variables\n",
    "    coordinates, ratings, reviews, is_opens = np.split(dataset, [2, 3, 4], axis = 1)\n",
    "    one_hot_array = np.zeros((ratings.shape[0], categorical_dim))\n",
    "\n",
    "    for i, r in enumerate(ratings):\n",
    "        one_hot_array[i][categorical_map[r[0]]] = 1\n",
    "    \n",
    "    # handling coordinates\n",
    "    if scaler_to_use is None:\n",
    "        scaler_to_use = preprocessing.StandardScaler()\n",
    "        scaler_to_use.fit(coordinates)\n",
    "    \n",
    "    coordinates = scaler_to_use.transform(coordinates)\n",
    "    \n",
    "    if save_scaler:\n",
    "        dump(scaler_to_use, open('./model/standard_scaler.pkl', 'wb'))\n",
    "    \n",
    "    \n",
    "    \n",
    "#     for i, c in enumerate(coordinates):\n",
    "#         coordinates[i][0] = (c[0]+180)/360\n",
    "#         coordinates[i][1] = (c[1]+180)/360\n",
    "\n",
    "    dataset = np.concatenate((coordinates, reviews, is_opens, one_hot_array), axis = 1)\n",
    "\n",
    "    # creating the labels\n",
    "    coordinates_labels = coordinates\n",
    "    review_labels = reviews\n",
    "    categorical_labels = one_hot_array\n",
    "    binary_labels = is_opens\n",
    "    return dataset, coordinates_labels, review_labels, categorical_labels, binary_labels\n",
    "\n",
    "train_dataset, train_coordinates_labels, train_review_labels, train_categorical_labels, train_binary_labels = format_data(train_set)\n",
    "\n",
    "## Open scaler\n",
    "scaler = load(open('./model/standard_scaler.pkl', 'rb'))\n",
    "test_dataset, test_coordinates_labels, test_review_labels, test_categorical_labels, test_binary_labels = format_data(test_set, scaler, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Review_Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(train_review_labels, 1000)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 174565 samples, validate on 1 samples\n",
      "Epoch 1/50\n",
      "174565/174565 [==============================] - 17s 97us/sample - loss: -89.9459 - x_pred_binary_loss: 0.5043 - x_pred_categorical_loss: 2.1485 - x_pred_review_log_lambda_layer_loss: -98.5309 - x_pred_coordinates_mdn_layer_loss: 1.8392 - val_loss: -43.6719 - val_x_pred_binary_loss: 0.1467 - val_x_pred_categorical_loss: 1.4049 - val_x_pred_review_log_lambda_layer_loss: -45.8876 - val_x_pred_coordinates_mdn_layer_loss: -0.1929\n",
      "Epoch 2/50\n",
      "174565/174565 [==============================] - 10s 57us/sample - loss: -101.3797 - x_pred_binary_loss: 0.4547 - x_pred_categorical_loss: 2.0217 - x_pred_review_log_lambda_layer_loss: -107.9093 - x_pred_coordinates_mdn_layer_loss: 0.6540 - val_loss: -41.9807 - val_x_pred_binary_loss: 0.1930 - val_x_pred_categorical_loss: 1.4972 - val_x_pred_review_log_lambda_layer_loss: -45.4995 - val_x_pred_coordinates_mdn_layer_loss: -1.7144\n",
      "Epoch 3/50\n",
      "174565/174565 [==============================] - 10s 56us/sample - loss: -103.1918 - x_pred_binary_loss: 0.4414 - x_pred_categorical_loss: 1.9908 - x_pred_review_log_lambda_layer_loss: -108.1635 - x_pred_coordinates_mdn_layer_loss: -0.5517 - val_loss: -43.4466 - val_x_pred_binary_loss: 0.1619 - val_x_pred_categorical_loss: 1.4504 - val_x_pred_review_log_lambda_layer_loss: -45.0545 - val_x_pred_coordinates_mdn_layer_loss: -4.1799\n",
      "Epoch 4/50\n",
      "174565/174565 [==============================] - 11s 62us/sample - loss: -104.2186 - x_pred_binary_loss: 0.4387 - x_pred_categorical_loss: 1.9781 - x_pred_review_log_lambda_layer_loss: -108.2686 - x_pred_coordinates_mdn_layer_loss: -1.2293 - val_loss: -44.9433 - val_x_pred_binary_loss: 0.1598 - val_x_pred_categorical_loss: 1.6895 - val_x_pred_review_log_lambda_layer_loss: -42.3855 - val_x_pred_coordinates_mdn_layer_loss: -4.4560\n",
      "Epoch 5/50\n",
      "174565/174565 [==============================] - 13s 76us/sample - loss: -104.9342 - x_pred_binary_loss: 0.4389 - x_pred_categorical_loss: 1.9724 - x_pred_review_log_lambda_layer_loss: -108.2836 - x_pred_coordinates_mdn_layer_loss: -1.7368 - val_loss: -44.4782 - val_x_pred_binary_loss: 0.1539 - val_x_pred_categorical_loss: 1.4771 - val_x_pred_review_log_lambda_layer_loss: -45.9071 - val_x_pred_coordinates_mdn_layer_loss: -4.1574\n",
      "Epoch 6/50\n",
      "174565/174565 [==============================] - 12s 68us/sample - loss: -105.7647 - x_pred_binary_loss: 0.4388 - x_pred_categorical_loss: 1.9714 - x_pred_review_log_lambda_layer_loss: -108.3170 - x_pred_coordinates_mdn_layer_loss: -2.4850 - val_loss: -45.7926 - val_x_pred_binary_loss: 0.1634 - val_x_pred_categorical_loss: 1.4771 - val_x_pred_review_log_lambda_layer_loss: -45.9181 - val_x_pred_coordinates_mdn_layer_loss: -5.2657\n",
      "Epoch 7/50\n",
      "174565/174565 [==============================] - 10s 60us/sample - loss: -106.1066 - x_pred_binary_loss: 0.4386 - x_pred_categorical_loss: 1.9706 - x_pred_review_log_lambda_layer_loss: -108.3469 - x_pred_coordinates_mdn_layer_loss: -2.7287 - val_loss: -46.7188 - val_x_pred_binary_loss: 0.1854 - val_x_pred_categorical_loss: 1.5499 - val_x_pred_review_log_lambda_layer_loss: -44.1957 - val_x_pred_coordinates_mdn_layer_loss: -5.2947\n",
      "Epoch 8/50\n",
      "174565/174565 [==============================] - 19s 108us/sample - loss: -106.2522 - x_pred_binary_loss: 0.4387 - x_pred_categorical_loss: 1.9704 - x_pred_review_log_lambda_layer_loss: -108.3320 - x_pred_coordinates_mdn_layer_loss: -2.8521 - val_loss: -44.8391 - val_x_pred_binary_loss: 0.1697 - val_x_pred_categorical_loss: 1.6087 - val_x_pred_review_log_lambda_layer_loss: -44.6553 - val_x_pred_coordinates_mdn_layer_loss: -4.4626\n",
      "Epoch 9/50\n",
      "174565/174565 [==============================] - 10s 59us/sample - loss: -106.3705 - x_pred_binary_loss: 0.4386 - x_pred_categorical_loss: 1.9698 - x_pred_review_log_lambda_layer_loss: -108.3740 - x_pred_coordinates_mdn_layer_loss: -2.9307 - val_loss: -45.3144 - val_x_pred_binary_loss: 0.1519 - val_x_pred_categorical_loss: 1.4543 - val_x_pred_review_log_lambda_layer_loss: -45.1225 - val_x_pred_coordinates_mdn_layer_loss: -5.0652\n",
      "Epoch 10/50\n",
      "174565/174565 [==============================] - 11s 62us/sample - loss: -106.4630 - x_pred_binary_loss: 0.4386 - x_pred_categorical_loss: 1.9694 - x_pred_review_log_lambda_layer_loss: -108.3581 - x_pred_coordinates_mdn_layer_loss: -2.9886 - val_loss: -46.3805 - val_x_pred_binary_loss: 0.1748 - val_x_pred_categorical_loss: 1.4602 - val_x_pred_review_log_lambda_layer_loss: -45.8259 - val_x_pred_coordinates_mdn_layer_loss: -5.0245\n",
      "Epoch 11/50\n",
      "174565/174565 [==============================] - 13s 73us/sample - loss: -106.5024 - x_pred_binary_loss: 0.4386 - x_pred_categorical_loss: 1.9695 - x_pred_review_log_lambda_layer_loss: -108.3448 - x_pred_coordinates_mdn_layer_loss: -3.0269 - val_loss: -45.7048 - val_x_pred_binary_loss: 0.1578 - val_x_pred_categorical_loss: 1.5272 - val_x_pred_review_log_lambda_layer_loss: -45.9355 - val_x_pred_coordinates_mdn_layer_loss: -4.7772\n",
      "Epoch 12/50\n",
      "174565/174565 [==============================] - 14s 81us/sample - loss: -106.5846 - x_pred_binary_loss: 0.4386 - x_pred_categorical_loss: 1.9691 - x_pred_review_log_lambda_layer_loss: -108.3828 - x_pred_coordinates_mdn_layer_loss: -3.0528 - val_loss: -46.6001 - val_x_pred_binary_loss: 0.1518 - val_x_pred_categorical_loss: 1.3951 - val_x_pred_review_log_lambda_layer_loss: -42.4058 - val_x_pred_coordinates_mdn_layer_loss: -4.7661\n",
      "Epoch 13/50\n",
      "174565/174565 [==============================] - 14s 80us/sample - loss: -106.4855 - x_pred_binary_loss: 0.4386 - x_pred_categorical_loss: 1.9681 - x_pred_review_log_lambda_layer_loss: -108.2659 - x_pred_coordinates_mdn_layer_loss: -3.0798 - val_loss: -46.2648 - val_x_pred_binary_loss: 0.1631 - val_x_pred_categorical_loss: 1.6307 - val_x_pred_review_log_lambda_layer_loss: -44.4382 - val_x_pred_coordinates_mdn_layer_loss: -4.6915\n",
      "Epoch 14/50\n",
      "174565/174565 [==============================] - 12s 70us/sample - loss: -106.6132 - x_pred_binary_loss: 0.4384 - x_pred_categorical_loss: 1.9688 - x_pred_review_log_lambda_layer_loss: -108.3534 - x_pred_coordinates_mdn_layer_loss: -3.0904 - val_loss: -45.6628 - val_x_pred_binary_loss: 0.1546 - val_x_pred_categorical_loss: 1.4816 - val_x_pred_review_log_lambda_layer_loss: -45.7091 - val_x_pred_coordinates_mdn_layer_loss: -4.8832\n",
      "Epoch 15/50\n",
      "174565/174565 [==============================] - 12s 68us/sample - loss: -106.6572 - x_pred_binary_loss: 0.4383 - x_pred_categorical_loss: 1.9687 - x_pred_review_log_lambda_layer_loss: -108.3896 - x_pred_coordinates_mdn_layer_loss: -3.1038 - val_loss: -46.1324 - val_x_pred_binary_loss: 0.1657 - val_x_pred_categorical_loss: 1.4872 - val_x_pred_review_log_lambda_layer_loss: -45.9989 - val_x_pred_coordinates_mdn_layer_loss: -4.8958\n",
      "Epoch 16/50\n",
      "174565/174565 [==============================] - 12s 70us/sample - loss: -106.6772 - x_pred_binary_loss: 0.4385 - x_pred_categorical_loss: 1.9689 - x_pred_review_log_lambda_layer_loss: -108.3900 - x_pred_coordinates_mdn_layer_loss: -3.1149 - val_loss: -45.6843 - val_x_pred_binary_loss: 0.1645 - val_x_pred_categorical_loss: 1.4148 - val_x_pred_review_log_lambda_layer_loss: -45.0758 - val_x_pred_coordinates_mdn_layer_loss: -4.8641\n",
      "Epoch 17/50\n",
      "174565/174565 [==============================] - 10s 57us/sample - loss: -106.6983 - x_pred_binary_loss: 0.4384 - x_pred_categorical_loss: 1.9690 - x_pred_review_log_lambda_layer_loss: -108.3880 - x_pred_coordinates_mdn_layer_loss: -3.1196 - val_loss: -46.9492 - val_x_pred_binary_loss: 0.1716 - val_x_pred_categorical_loss: 1.5555 - val_x_pred_review_log_lambda_layer_loss: -44.7691 - val_x_pred_coordinates_mdn_layer_loss: -4.5667\n",
      "Epoch 18/50\n",
      "174565/174565 [==============================] - 11s 63us/sample - loss: -106.6885 - x_pred_binary_loss: 0.4384 - x_pred_categorical_loss: 1.9688 - x_pred_review_log_lambda_layer_loss: -108.3835 - x_pred_coordinates_mdn_layer_loss: -3.1271 - val_loss: -46.4157 - val_x_pred_binary_loss: 0.1672 - val_x_pred_categorical_loss: 1.5319 - val_x_pred_review_log_lambda_layer_loss: -45.7509 - val_x_pred_coordinates_mdn_layer_loss: -5.1984\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174565/174565 [==============================] - 11s 66us/sample - loss: -106.7232 - x_pred_binary_loss: 0.4384 - x_pred_categorical_loss: 1.9689 - x_pred_review_log_lambda_layer_loss: -108.4129 - x_pred_coordinates_mdn_layer_loss: -3.1339 - val_loss: -45.8126 - val_x_pred_binary_loss: 0.1638 - val_x_pred_categorical_loss: 1.4591 - val_x_pred_review_log_lambda_layer_loss: -45.9771 - val_x_pred_coordinates_mdn_layer_loss: -4.7239\n",
      "Epoch 20/50\n",
      "174565/174565 [==============================] - 11s 61us/sample - loss: -106.7247 - x_pred_binary_loss: 0.4384 - x_pred_categorical_loss: 1.9688 - x_pred_review_log_lambda_layer_loss: -108.3963 - x_pred_coordinates_mdn_layer_loss: -3.1375 - val_loss: -46.3860 - val_x_pred_binary_loss: 0.1589 - val_x_pred_categorical_loss: 1.4458 - val_x_pred_review_log_lambda_layer_loss: -45.5334 - val_x_pred_coordinates_mdn_layer_loss: -4.8681\n",
      "Epoch 21/50\n",
      "174565/174565 [==============================] - 11s 62us/sample - loss: -106.7286 - x_pred_binary_loss: 0.4384 - x_pred_categorical_loss: 1.9684 - x_pred_review_log_lambda_layer_loss: -108.3901 - x_pred_coordinates_mdn_layer_loss: -3.1465 - val_loss: -46.0362 - val_x_pred_binary_loss: 0.1637 - val_x_pred_categorical_loss: 1.5531 - val_x_pred_review_log_lambda_layer_loss: -45.5942 - val_x_pred_coordinates_mdn_layer_loss: -4.9375\n",
      "Epoch 22/50\n",
      "174565/174565 [==============================] - 10s 57us/sample - loss: -106.7063 - x_pred_binary_loss: 0.4385 - x_pred_categorical_loss: 1.9688 - x_pred_review_log_lambda_layer_loss: -108.3540 - x_pred_coordinates_mdn_layer_loss: -3.1508 - val_loss: -46.0406 - val_x_pred_binary_loss: 0.1649 - val_x_pred_categorical_loss: 1.3959 - val_x_pred_review_log_lambda_layer_loss: -42.4430 - val_x_pred_coordinates_mdn_layer_loss: -4.7549\n",
      "Epoch 23/50\n",
      "174565/174565 [==============================] - 10s 55us/sample - loss: -106.6678 - x_pred_binary_loss: 0.4384 - x_pred_categorical_loss: 1.9681 - x_pred_review_log_lambda_layer_loss: -108.3133 - x_pred_coordinates_mdn_layer_loss: -3.1524 - val_loss: -45.8843 - val_x_pred_binary_loss: 0.1591 - val_x_pred_categorical_loss: 1.5679 - val_x_pred_review_log_lambda_layer_loss: -45.0646 - val_x_pred_coordinates_mdn_layer_loss: -4.9005\n",
      "Epoch 24/50\n",
      "174565/174565 [==============================] - 9s 52us/sample - loss: -106.7513 - x_pred_binary_loss: 0.4385 - x_pred_categorical_loss: 1.9681 - x_pred_review_log_lambda_layer_loss: -108.3862 - x_pred_coordinates_mdn_layer_loss: -3.1560 - val_loss: -46.2122 - val_x_pred_binary_loss: 0.1626 - val_x_pred_categorical_loss: 1.4502 - val_x_pred_review_log_lambda_layer_loss: -45.3149 - val_x_pred_coordinates_mdn_layer_loss: -4.5360\n",
      "Epoch 25/50\n",
      "174565/174565 [==============================] - 9s 50us/sample - loss: -106.7819 - x_pred_binary_loss: 0.4383 - x_pred_categorical_loss: 1.9682 - x_pred_review_log_lambda_layer_loss: -108.4154 - x_pred_coordinates_mdn_layer_loss: -3.1546 - val_loss: -45.9645 - val_x_pred_binary_loss: 0.1730 - val_x_pred_categorical_loss: 1.5985 - val_x_pred_review_log_lambda_layer_loss: -44.2971 - val_x_pred_coordinates_mdn_layer_loss: -4.8282\n",
      "Epoch 26/50\n",
      "174565/174565 [==============================] - 9s 49us/sample - loss: -106.7685 - x_pred_binary_loss: 0.4383 - x_pred_categorical_loss: 1.9687 - x_pred_review_log_lambda_layer_loss: -108.4001 - x_pred_coordinates_mdn_layer_loss: -3.1584 - val_loss: -46.3129 - val_x_pred_binary_loss: 0.1621 - val_x_pred_categorical_loss: 1.4463 - val_x_pred_review_log_lambda_layer_loss: -45.6451 - val_x_pred_coordinates_mdn_layer_loss: -4.9612\n",
      "Epoch 27/50\n",
      "174565/174565 [==============================] - 11s 63us/sample - loss: -106.7485 - x_pred_binary_loss: 0.4383 - x_pred_categorical_loss: 1.9686 - x_pred_review_log_lambda_layer_loss: -108.3748 - x_pred_coordinates_mdn_layer_loss: -3.1577 - val_loss: -46.3817 - val_x_pred_binary_loss: 0.1644 - val_x_pred_categorical_loss: 1.5053 - val_x_pred_review_log_lambda_layer_loss: -45.6705 - val_x_pred_coordinates_mdn_layer_loss: -5.1153\n",
      "Epoch 28/50\n",
      "141100/174565 [=======================>......] - ETA: 1s - loss: -105.5247 - x_pred_binary_loss: 0.4380 - x_pred_categorical_loss: 1.9692 - x_pred_review_log_lambda_layer_loss: -107.1377 - x_pred_coordinates_mdn_layer_loss: -3.1697"
     ]
    }
   ],
   "source": [
    "vae.fit(train_dataset , [train_binary_labels, train_categorical_labels, train_review_labels, train_coordinates_labels], shuffle = True, epochs = epochs, batch_size = batch_size, validation_data=(test_dataset , [test_binary_labels, test_categorical_labels, test_review_labels, test_coordinates_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_input = Input(shape=(latent_dim, ), name = 'decode_input')\n",
    "decode_layer_1 = decode_1(decode_input)\n",
    "decode_layer_2 = decode_2(decode_layer_1)\n",
    "\n",
    "decode_x_pred_coordinates=x_pred_coordinates_mdn_layer(decode_layer_2)\n",
    "# x_pred_coordinates_mu = x_pred_coordinates_mu_layer(decode_layer_2)\n",
    "# x_pred_coordinates_log_var = x_pred_coordinates_log_var_layer(decode_layer_2)\n",
    "# decode_x_pred_coordinates = x_pred_coordinates_layer([x_pred_coordinates_mu, x_pred_coordinates_log_var])\n",
    "\n",
    "decode_x_pred_review = x_pred_review_log_lambda_layer(decode_layer_2)\n",
    "\n",
    "decode_x_pred_binary = x_pred_binary_layer(decode_layer_2) # binary cross entropy\n",
    "decode_x_pred_categorical = x_pred_categorical_layer(decode_layer_2) # categorical cross entropy\n",
    "\n",
    "\n",
    "decoder = Model(decode_input, [decode_x_pred_coordinates, decode_x_pred_review, decode_x_pred_categorical, decode_x_pred_binary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"503pt\" viewBox=\"0.00 0.00 1709.16 377.00\" width=\"2279pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(0.75 0.75) rotate(0) translate(4 373)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-373 1705.16,-373 1705.16,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 5358903704 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>5358903704</title>\n",
       "<polygon fill=\"none\" points=\"760.83,-324.5 760.83,-368.5 1047.16,-368.5 1047.16,-324.5 760.83,-324.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"841.34\" y=\"-342.3\">decode_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"921.84,-324.5 921.84,-368.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"949.67\" y=\"-353.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"921.84,-346.5 977.51,-346.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"949.67\" y=\"-331.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"977.51,-324.5 977.51,-368.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1012.33\" y=\"-353.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"977.51,-346.5 1047.16,-346.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1012.33\" y=\"-331.3\">(None, 4)</text>\n",
       "</g>\n",
       "<!-- 5296621552 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>5296621552</title>\n",
       "<polygon fill=\"none\" points=\"769.38,-243.5 769.38,-287.5 1038.61,-287.5 1038.61,-243.5 769.38,-243.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"837.84\" y=\"-261.3\">hidden_dec_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"906.29,-243.5 906.29,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"934.12\" y=\"-272.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"906.29,-265.5 961.96,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"934.12\" y=\"-250.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"961.96,-243.5 961.96,-287.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1000.28\" y=\"-272.3\">(None, 4)</text>\n",
       "<polyline fill=\"none\" points=\"961.96,-265.5 1038.61,-265.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1000.28\" y=\"-250.3\">(None, 50)</text>\n",
       "</g>\n",
       "<!-- 5358903704&#45;&gt;5296621552 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>5358903704-&gt;5296621552</title>\n",
       "<path d=\"M904,-324.33C904,-316.18 904,-306.7 904,-297.8\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"907.5,-297.73 904,-287.73 900.5,-297.73 907.5,-297.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5296684896 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>5296684896</title>\n",
       "<polygon fill=\"none\" points=\"769.38,-162.5 769.38,-206.5 1038.61,-206.5 1038.61,-162.5 769.38,-162.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"837.84\" y=\"-180.3\">hidden_dec_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"906.29,-162.5 906.29,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"934.12\" y=\"-191.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"906.29,-184.5 961.96,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"934.12\" y=\"-169.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"961.96,-162.5 961.96,-206.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1000.28\" y=\"-191.3\">(None, 50)</text>\n",
       "<polyline fill=\"none\" points=\"961.96,-184.5 1038.61,-184.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1000.28\" y=\"-169.3\">(None, 50)</text>\n",
       "</g>\n",
       "<!-- 5296621552&#45;&gt;5296684896 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>5296621552-&gt;5296684896</title>\n",
       "<path d=\"M904,-243.33C904,-235.18 904,-225.7 904,-216.8\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"907.5,-216.73 904,-206.73 900.5,-216.73 907.5,-216.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5296763568 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>5296763568</title>\n",
       "<polygon fill=\"none\" points=\"0,-81.5 0,-125.5 325.99,-125.5 325.99,-81.5 0,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"96.84\" y=\"-99.3\">x_pred_coordinates_mu: Dense</text>\n",
       "<polyline fill=\"none\" points=\"193.67,-81.5 193.67,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.51\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"193.67,-103.5 249.34,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"221.51\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"249.34,-81.5 249.34,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287.67\" y=\"-110.3\">(None, 50)</text>\n",
       "<polyline fill=\"none\" points=\"249.34,-103.5 325.99,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"287.67\" y=\"-88.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 5296684896&#45;&gt;5296763568 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>5296684896-&gt;5296763568</title>\n",
       "<path d=\"M769.1,-170.77C656.23,-159.91 490.65,-143.49 336.53,-126.16\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"336.73,-122.66 326.4,-125.02 335.95,-129.62 336.73,-122.66\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5296765976 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>5296765976</title>\n",
       "<polygon fill=\"none\" points=\"343.56,-81.5 343.56,-125.5 694.43,-125.5 694.43,-81.5 343.56,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"452.84\" y=\"-99.3\">x_pred_coordinates_log_var: Dense</text>\n",
       "<polyline fill=\"none\" points=\"562.11,-81.5 562.11,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"589.95\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"562.11,-103.5 617.78,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"589.95\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"617.78,-81.5 617.78,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"656.1\" y=\"-110.3\">(None, 50)</text>\n",
       "<polyline fill=\"none\" points=\"617.78,-103.5 694.43,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"656.1\" y=\"-88.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 5296684896&#45;&gt;5296765976 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>5296684896-&gt;5296765976</title>\n",
       "<path d=\"M801.58,-162.49C749.15,-151.73 685.21,-138.61 631.43,-127.57\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"632,-124.12 621.51,-125.53 630.6,-130.97 632,-124.12\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5296797344 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>5296797344</title>\n",
       "<polygon fill=\"none\" points=\"712.63,-81.5 712.63,-125.5 1095.36,-125.5 1095.36,-81.5 712.63,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"837.84\" y=\"-99.3\">x_pred_review_log_lambda_layer: Dense</text>\n",
       "<polyline fill=\"none\" points=\"963.05,-81.5 963.05,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"990.88\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"963.05,-103.5 1018.71,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"990.88\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1018.71,-81.5 1018.71,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1057.04\" y=\"-110.3\">(None, 50)</text>\n",
       "<polyline fill=\"none\" points=\"1018.71,-103.5 1095.36,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1057.04\" y=\"-88.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 5296684896&#45;&gt;5296797344 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>5296684896-&gt;5296797344</title>\n",
       "<path d=\"M904,-162.33C904,-154.18 904,-144.7 904,-135.8\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"907.5,-135.73 904,-125.73 900.5,-135.73 907.5,-135.73\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5296797736 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>5296797736</title>\n",
       "<polygon fill=\"none\" points=\"1113.01,-81.5 1113.01,-125.5 1410.98,-125.5 1410.98,-81.5 1113.01,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1195.84\" y=\"-99.3\">x_pred_categorical: Dense</text>\n",
       "<polyline fill=\"none\" points=\"1278.66,-81.5 1278.66,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1306.5\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1278.66,-103.5 1334.33,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1306.5\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1334.33,-81.5 1334.33,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1372.66\" y=\"-110.3\">(None, 50)</text>\n",
       "<polyline fill=\"none\" points=\"1334.33,-103.5 1410.98,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1372.4\" y=\"-88.3\">(None, 11)</text>\n",
       "</g>\n",
       "<!-- 5296684896&#45;&gt;5296797736 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>5296684896-&gt;5296797736</title>\n",
       "<path d=\"M999.46,-162.43C1048.01,-151.72 1107.12,-138.68 1156.96,-127.68\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1157.78,-131.08 1166.79,-125.51 1156.27,-124.25 1157.78,-131.08\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5296798968 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>5296798968</title>\n",
       "<polygon fill=\"none\" points=\"1428.83,-81.5 1428.83,-125.5 1701.16,-125.5 1701.16,-81.5 1428.83,-81.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1498.84\" y=\"-99.3\">x_pred_binary: Dense</text>\n",
       "<polyline fill=\"none\" points=\"1568.85,-81.5 1568.85,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1596.68\" y=\"-110.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"1568.85,-103.5 1624.51,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1596.68\" y=\"-88.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"1624.51,-81.5 1624.51,-125.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1662.84\" y=\"-110.3\">(None, 50)</text>\n",
       "<polyline fill=\"none\" points=\"1624.51,-103.5 1701.16,-103.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"1662.84\" y=\"-88.3\">(None, 1)</text>\n",
       "</g>\n",
       "<!-- 5296684896&#45;&gt;5296798968 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>5296684896-&gt;5296798968</title>\n",
       "<path d=\"M1038.88,-169.82C1140.53,-159.19 1283.64,-143.55 1418.67,-126.15\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"1419.32,-129.6 1428.79,-124.84 1418.42,-122.66 1419.32,-129.6\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5296766088 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>5296766088</title>\n",
       "<polygon fill=\"none\" points=\"141.86,-0.5 141.86,-44.5 540.13,-44.5 540.13,-0.5 141.86,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"243.35\" y=\"-18.3\">x_pred_coordinates: Concatenate</text>\n",
       "<polyline fill=\"none\" points=\"344.84,-0.5 344.84,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.68\" y=\"-29.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"344.84,-22.5 400.51,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"372.68\" y=\"-7.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"400.51,-0.5 400.51,-44.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"470.32\" y=\"-29.3\">[(None, 2), (None, 2)]</text>\n",
       "<polyline fill=\"none\" points=\"400.51,-22.5 540.13,-22.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"469.83\" y=\"-7.3\">(None, 4)</text>\n",
       "</g>\n",
       "<!-- 5296763568&#45;&gt;5296766088 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>5296763568-&gt;5296766088</title>\n",
       "<path d=\"M210.7,-81.33C233.33,-71.28 260.55,-59.2 284.24,-48.69\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"285.75,-51.85 293.47,-44.59 282.91,-45.45 285.75,-51.85\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5296765976&#45;&gt;5296766088 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>5296765976-&gt;5296766088</title>\n",
       "<path d=\"M471.29,-81.33C448.66,-71.28 421.44,-59.2 397.75,-48.69\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"399.08,-45.45 388.52,-44.59 396.24,-51.85 399.08,-45.45\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(decoder, show_shapes=True)\n",
    "    .create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Models and Metadata\n",
    "#### Saving Models (Actually, only the decoder matter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n",
      "Saved model to disk\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = vae.to_json()\n",
    "with open(\"./model/vae_full_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "vae.save_weights(\"./model/vae_full_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "encoder = Model(x, [z_mu, z_log_var])\n",
    "model_json = encoder.to_json()\n",
    "with open(\"./model/vae_encoder.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "encoder.save_weights(\"./model/vae_encoder.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "model_json = decoder.to_json()\n",
    "with open(\"./model/vae_decoder.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "decoder.save_weights(\"./model/vae_decoder.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating and Saving Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_sample = train_dataset[np.random.choice(len(train_dataset), size=10000, replace=False)]\n",
    "vae_sample_mu, vae_sample_log_var = encoder.predict(vae_sample, batch_size=batch_size)\n",
    "np.save('./model/sample_mu.npy', vae_sample_mu)\n",
    "np.save('./model/sample_log_var.npy', vae_sample_log_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating some Samples for Testing\n",
    "#### Functions for data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_categorical_map = {0:0, 1:0.5, 2:1, 3:1.5, 4:2, 5:2.5, 6:3, 7:3.5, 8:4, 9:4.5, 10:5}\n",
    "def sample(model, input_mu, input_log_var, samples_per_z=1):\n",
    "    multiplied_input_mu = np.repeat(input_mu, samples_per_z, axis=0)\n",
    "    multiplied_input_log_var = np.repeat(input_log_var, samples_per_z, axis=0)\n",
    "    eps = np.random.normal(size=(multiplied_input_mu.shape[0], latent_dim))\n",
    "    z = reparameterize(multiplied_input_mu, multiplied_input_log_var, eps)\n",
    "    predictions = model.predict(z, batch_size = None, steps = 1)\n",
    "    return reconstruct(predictions)\n",
    "    \n",
    "def reconstruct(predictions):\n",
    "    coordinates, review, categorical, binary = predictions\n",
    "    \n",
    "    # coordinates handled here\n",
    "    y_samples = np.apply_along_axis(sample_from_output, 1, y_test, OUTPUT_DIMS, N_MIXES, temp=1.0)\n",
    "    \n",
    "    mu, log_var = np.split(coordinates, indices_or_sections = 2,axis = 1)\n",
    "    eps = np.random.normal(size=mu.shape)\n",
    "    coordinates_data = reparameterize(mu, log_var, eps)\n",
    "    scaler = load(open('./model/standard_scaler.pkl', 'rb'))\n",
    "    coordinates_data = scaler.inverse_transform(coordinates_data)\n",
    "    \n",
    "    \n",
    "    for i, c in enumerate(coordinates_data):\n",
    "        if c[0] > 180.0:\n",
    "            coordinates_data[i][0]= 180.0\n",
    "        if c[0] < -180.0:\n",
    "            coordinates_data[i][0]= -180.0\n",
    "        if c[1] > 180.0:\n",
    "            coordinates_data[i][1]= 180.0\n",
    "        if c[1] < -180.0:\n",
    "            coordinates_data[i][1]= -180.0\n",
    "    \n",
    "    ## review_count handled here\n",
    "    exp_log_review = np.exp(review)\n",
    "    review_data = np.random.poisson(lam=exp_log_review, size = review.shape)\n",
    "    for i, r in enumerate(review_data):\n",
    "        if r[0] < 0:\n",
    "            review_data[i][0] = 0\n",
    "        review_data[i][0] = float(int(review_data[i][0]))\n",
    "    \n",
    "    categorical = np.apply_along_axis(lambda t : np.random.multinomial(1,t), -1, categorical)\n",
    "    categorical = np.apply_along_axis(lambda t : np.argmax(t), -1, categorical)\n",
    "    categorical = np.expand_dims(categorical, axis = -1)\n",
    "    categorical_data = np.apply_along_axis(lambda t : float(reverse_categorical_map[t[0]]), -1, categorical)\n",
    "    categorical_data = np.expand_dims(categorical_data, axis = -1)\n",
    "    binary_data = np.apply_along_axis(lambda t: np.random.binomial(1, t), -1, binary)\n",
    "#     coordinates, reviews = np.split(continuous_data, indices_or_sections=[2], axis = 1)\n",
    "    return np.concatenate([coordinates_data, categorical_data, review_data, binary_data], axis = 1)\n",
    "    \n",
    "\n",
    "def reparameterize(input_mu, input_log_var, eps):\n",
    "    sigma = np.exp(0.5*input_log_var)\n",
    "    return eps*sigma + input_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Samples\n",
    "Make use of the funciton sample to generate samples with our model. U need to supply an array of mu and their respective log var in a separate array to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_samples = sample(decoder, vae_sample_mu, vae_sample_log_var)\n",
    "\n",
    "# Saving the samples in a separate file\n",
    "file_name = './samples/vae_6_sample_' + str(len(vae_samples)) + '.csv'\n",
    "np.savetxt(file_name, vae_samples, delimiter = ',', header='latitude,longitude,stars,review_count,is_open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  42.00038484,  -85.10048955,    4.5       ,    4.        ,\n",
       "           1.        ],\n",
       "       [  43.30385621,  -80.81788208,    3.5       ,    8.        ,\n",
       "           1.        ],\n",
       "       [  33.68691703, -109.71916511,    5.        ,   14.        ,\n",
       "           1.        ],\n",
       "       [  33.41740956, -112.11357711,    5.        ,   30.        ,\n",
       "           1.        ],\n",
       "       [  41.67314222,  -79.53445609,    4.        ,   62.        ,\n",
       "           1.        ],\n",
       "       [  42.99318417,  -79.87686542,    2.        ,    4.        ,\n",
       "           1.        ],\n",
       "       [  33.45580296, -111.39862833,    3.        ,    2.        ,\n",
       "           1.        ],\n",
       "       [  36.63807986,  -80.99035687,    4.        ,    9.        ,\n",
       "           1.        ],\n",
       "       [  33.65037958, -112.2049321 ,    3.        ,   13.        ,\n",
       "           1.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_samples[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
